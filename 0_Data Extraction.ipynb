{"cells":[{"cell_type":"markdown","metadata":{"id":"5DTIqC-JopP9"},"source":["# Data Extraction\n","\n","\n","---\n","\n","\n","**Mentor:**\n","  - ***Professor Richard Sowers***, Department of Industrial and Systems Engineering, University of Illinois at Urbana-Champaign (UIUC).\n","\n","**Group Members:**\n","  - ***Advika Pattiwar*** (linkedin.com/in/advika-pattiwar)\n","  - ***Dhruv Borda*** (linkedin.com/thebordadhruv)\n","  - ***Hrithik Rathi*** (linkedin.com/in/hrithik-rathi)\n","  - ***Suvrata Gayathri Kappagantula*** (linkedin.com/in/gayathrikappagantula)\n","\n","\n","---\n","\n","\n","In this project, we will be working with two datasets: a debugging dataset and a working dataset. It's important to understand their characteristics and how we will handle them.\n","\n","**Primary Goals:**\n","\n","In this project, we will be working with two datasets: a debugging dataset and a working dataset. It's important to understand their characteristics and how we will handle them.\n","\n","1. **Debugging Dataset**\n","   - The debugging dataset is intentionally kept small. It's designed for testing our code efficiently, and reasonable code should run on it in about 2 minutes.\n","\n","2. **Working Dataset**\n","   - The working dataset, on the other hand, is the main dataset we'll use for our project. It's larger and more representative of the problem we're tackling. However, we need to ensure that training on this dataset doesn't take excessively long, ideally, no more than 40 minutes.\n","\n","3. **Data Conversion to Pandas**\n","   - To start our project, we'll convert both datasets into Pandas DataFrames for ease of manipulation and analysis. Additionally, we'll pay special attention to datetime columns and convert them into Pandas timestamps. This conversion will enable us to work with time deltas and perform various time-related operations seamlessly.\n","\n","4. **Data Serialization with Pickle**\n","   - To optimize data loading and storage, we'll use the [`pandas.DataFrame.to_pickle`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html) method. This allows us to serialize our data into a binary file format, which can be loaded much faster and efficiently, while preserving the correct data types."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hmyAWQa97rE1","outputId":"6f7e8e44-17da-49b0-88e1-6f8688bc0f54"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting boto3\n","  Downloading boto3-1.29.6-py3-none-any.whl (135 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting botocore<1.33.0,>=1.32.6\n","  Downloading botocore-1.32.6-py3-none-any.whl (11.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.8.0,>=0.7.0\n","  Using cached s3transfer-0.7.0-py3-none-any.whl (79 kB)\n","Collecting python-dateutil<3.0.0,>=2.1\n","  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n","Collecting urllib3<2.1,>=1.25.4\n","  Downloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 kB\u001b[0m \u001b[31m799.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting six>=1.5\n","  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Installing collected packages: urllib3, six, jmespath, python-dateutil, botocore, s3transfer, boto3\n","Successfully installed boto3-1.29.6 botocore-1.32.6 jmespath-1.0.1 python-dateutil-2.8.2 s3transfer-0.7.0 six-1.16.0 urllib3-2.0.7\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install boto3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CcUqvP058bdo"},"outputs":[],"source":["import urllib.request\n","from zipfile import ZipFile\n","import re\n","\n","import os\n","import requests\n","\n","import io\n","from io import StringIO\n","from io import BytesIO\n","\n","import boto3\n","from botocore import UNSIGNED\n","from botocore.client import Config\n","\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"QL0BLnK7C_NO"},"source":["# Data Extraction"]},{"cell_type":"markdown","metadata":{"id":"4T4ww9sw4JBg"},"source":["## Method 1: Retrieving Data from External URLs\n","\n","- This method involves downloading data from external URLs, specifically CSV files contained within ZIP archives. It uses various libraries to fetch, unzip, and process the data, ultimately organizing it into separate DataFrames for different time periods based on the provided URLs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDSHpfSnZ3pg"},"outputs":[],"source":["# data_urls = [\n","#     \"https://s3.amazonaws.com/tripdata/202309-citibike-tripdata.csv.zip\",\n","#     \"https://s3.amazonaws.com/tripdata/202308-citibike-tripdata.csv.zip\",\n","#     \"https://s3.amazonaws.com/tripdata/202307-citibike-tripdata.csv.zip\",\n","#     \"https://s3.amazonaws.com/tripdata/202306-citibike-tripdata.csv.zip\",\n","#     \"https://s3.amazonaws.com/tripdata/202305-citibike-tripdata.csv.zip\",\n","#     \"https://s3.amazonaws.com/tripdata/202304-citibike-tripdata.csv.zip\"\n","# ]\n","\n","# data_frames = {}\n","\n","# for data_url in data_urls:\n","#     with urllib.request.urlopen(data_url) as url:\n","#         data = []\n","#         with ZipFile(BytesIO(url.read())) as my_zip_file:\n","#             for contained_file in my_zip_file.namelist():\n","#                 for line in my_zip_file.open(contained_file).readlines():\n","#                     s = str(line, 'unicode_escape')\n","#                     s = re.sub(r\"\\n\", \"\", s)\n","#                     s = re.sub(r\"\\\"\", \"\", s)\n","#                     line_s = s.split(\",\")\n","#                     data.append(line_s)\n","\n","#         month_year = re.search(r'(\\d{6})', data_url).group(1)\n","\n","#         df = pd.DataFrame(data)\n","\n","#         data_frames[month_year] = df\n","\n","# debugging_df = data_frames['202309'].sample(n=10000, random_state=1).copy()\n","# working_df = pd.concat([data_frames[key] for key in sorted(data_frames.keys(), reverse=True)], ignore_index=True).sample(n=100000, random_state=1)"]},{"cell_type":"markdown","metadata":{"id":"AZ_6SJGszbr3"},"source":["## Method 2: Retrieving Data from AWS S3 Bucket\n","\n","- This method entails fetching data directly from an AWS S3 bucket. AWS's Simple Storage Service (S3) provides a scalable object storage system, which is widely used for data storage and retrieval. In this method, we use the boto3 Python library to access and download data files stored in a specified S3 bucket. The data, once retrieved, is then read into a Pandas DataFrame for further analysis and processing."]},{"cell_type":"markdown","metadata":{"id":"GgozH90t-p86"},"source":["**Note:**\n","\n","For the datasets we've provided in this project:\n","- We have **not** allowed public users to directly import all the CSV files for both the \"debugging dataset\" and the \"working dataset\".\n","\n","- We've included a commented-out Python example below showing how one might import these CSVs. However, for better efficiency and user experience, we've chosen an alternative method.\n","\n","- Instead of raw CSVs, we've **preprocessed the data and stored it in pickle format**. This allows for faster loading and a reduced file size, offering benefits like:\n","  - **Efficiency**: Loading data from a pickle is generally faster than from a CSV.\n","  - **Size**: Pickle files can be more space-efficient.\n","  - **Simplicity**: Users can begin analyses without additional data wrangling.\n","\n","By following this approach, we provide direct access to the pickle files for both datasets.\n"]},{"cell_type":"markdown","metadata":{"id":"kOJilz46rb-t"},"source":["### Weather Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NnJM2jPWrezV"},"outputs":[],"source":["# AWS S3 bucket details\n","BUCKET_NAME = 'dhruvborda-project-nyccitibikerentals'\n","FILE_KEY = 'Dataset/Weather_DailySummaries.csv'\n","\n","# Initialize S3 client\n","s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n","\n","obj = s3.get_object(Bucket=BUCKET_NAME, Key=FILE_KEY)\n","DailyWeather = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))"]},{"cell_type":"markdown","metadata":{"id":"4hHDkQ7T8ieW"},"source":["### Debugging Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"58oIfp9x761q","outputId":"fc8a8195-e1a9-417f-8e59-c9d4d2c12085"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/r8/fk1w144x269f558nx98qf15r0000gn/T/ipykernel_87472/2874279095.py:21: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n","  data_frames[df_name] = pd.read_csv(StringIO(csv_body))\n"]},{"name":"stdout","output_type":"stream","text":["CSV file name: 202309-citibike-tripdata\n"]}],"source":["# AWS S3 bucket details\n","BUCKET_NAME = 'dhruvborda-project-nyccitibikerentals'\n","FOLDER_PATH = 'Dataset/Debugging Dataset/'\n","\n","# Initialize S3 client\n","s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n","\n","# List files in the specified S3 bucket directory\n","objects = s3.list_objects_v2(Bucket=BUCKET_NAME, Prefix=FOLDER_PATH)\n","\n","file_list = [content['Key'] for content in objects.get('Contents') if content['Key'].endswith('.csv')]\n","\n","data_frames = {}\n","\n","for file in file_list:\n","    # Read the S3 object directly into a pandas DataFrame\n","    csv_obj = s3.get_object(Bucket=BUCKET_NAME, Key=file)\n","    csv_body = csv_obj['Body'].read().decode('utf-8')\n","\n","    df_name = os.path.splitext(os.path.basename(file))[0]\n","    data_frames[df_name] = pd.read_csv(StringIO(csv_body))\n","\n","for csv_name in data_frames.keys():\n","    print(\"CSV file name:\", csv_name)\n","\n","debugging_df = pd.concat(data_frames.values(), ignore_index=True).sample(n=10000, random_state=1)"]},{"cell_type":"markdown","metadata":{"id":"eIdh-QMO8_G-"},"source":["### Working Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6BmopZXAE7P","outputId":"6766c7cc-54cd-4015-98f6-f5bf72b9b922"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/r8/fk1w144x269f558nx98qf15r0000gn/T/ipykernel_87472/2308568573.py:21: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n","  data_frames[df_name] = pd.read_csv(StringIO(csv_body))\n","/var/folders/r8/fk1w144x269f558nx98qf15r0000gn/T/ipykernel_87472/2308568573.py:21: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n","  data_frames[df_name] = pd.read_csv(StringIO(csv_body))\n","/var/folders/r8/fk1w144x269f558nx98qf15r0000gn/T/ipykernel_87472/2308568573.py:21: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n","  data_frames[df_name] = pd.read_csv(StringIO(csv_body))\n","/var/folders/r8/fk1w144x269f558nx98qf15r0000gn/T/ipykernel_87472/2308568573.py:21: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n","  data_frames[df_name] = pd.read_csv(StringIO(csv_body))\n","/var/folders/r8/fk1w144x269f558nx98qf15r0000gn/T/ipykernel_87472/2308568573.py:21: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n","  data_frames[df_name] = pd.read_csv(StringIO(csv_body))\n","/var/folders/r8/fk1w144x269f558nx98qf15r0000gn/T/ipykernel_87472/2308568573.py:21: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n","  data_frames[df_name] = pd.read_csv(StringIO(csv_body))\n"]},{"name":"stdout","output_type":"stream","text":["CSV file name: 202304-citibike-tripdata\n","CSV file name: 202305-citibike-tripdata\n","CSV file name: 202306-citibike-tripdata\n","CSV file name: 202307-citibike-tripdata\n","CSV file name: 202308-citibike-tripdata\n","CSV file name: 202309-citibike-tripdata\n"]}],"source":["# AWS S3 bucket details\n","BUCKET_NAME = 'dhruvborda-project-nyccitibikerentals'\n","FOLDER_PATH = 'Dataset/Working Dataset/'\n","\n","# Initialize S3 client\n","s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n","\n","# List files in the specified S3 bucket directory\n","objects = s3.list_objects_v2(Bucket=BUCKET_NAME, Prefix=FOLDER_PATH)\n","\n","file_list = [content['Key'] for content in objects.get('Contents') if content['Key'].endswith('.csv')]\n","\n","data_frames = {}\n","\n","for file in file_list:\n","    # Read the S3 object directly into a pandas DataFrame\n","    csv_obj = s3.get_object(Bucket=BUCKET_NAME, Key=file)\n","    csv_body = csv_obj['Body'].read().decode('utf-8')\n","\n","    df_name = os.path.splitext(os.path.basename(file))[0]\n","    data_frames[df_name] = pd.read_csv(StringIO(csv_body))\n","\n","for csv_name in data_frames.keys():\n","    print(\"CSV file name:\", csv_name)\n","\n","working_df = pd.concat(data_frames.values(), ignore_index=True).sample(n=1000000, random_state=1)"]},{"cell_type":"markdown","metadata":{"id":"iPXVvZ9yC7ef"},"source":["# Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"OonnxG2HDHpW"},"source":["## Weather Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aUPGrw57r9MV","outputId":"6a85111e-2a92-40ed-b3d1-bbca2d3f0d9d"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 183 entries, 0 to 182\n","Data columns (total 22 columns):\n"," #   Column     Non-Null Count  Dtype  \n","---  ------     --------------  -----  \n"," 0   STATION    183 non-null    object \n"," 1   NAME       183 non-null    object \n"," 2   LATITUDE   183 non-null    float64\n"," 3   LONGITUDE  183 non-null    float64\n"," 4   ELEVATION  183 non-null    float64\n"," 5   DATE       183 non-null    object \n"," 6   AWND       183 non-null    float64\n"," 7   PGTM       0 non-null      float64\n"," 8   PRCP       183 non-null    float64\n"," 9   SNOW       183 non-null    float64\n"," 10  SNWD       183 non-null    float64\n"," 11  TAVG       0 non-null      float64\n"," 12  TMAX       183 non-null    int64  \n"," 13  TMIN       183 non-null    int64  \n"," 14  WDF2       183 non-null    int64  \n"," 15  WDF5       182 non-null    float64\n"," 16  WSF2       183 non-null    float64\n"," 17  WSF5       182 non-null    float64\n"," 18  WT01       79 non-null     float64\n"," 19  WT02       8 non-null      float64\n"," 20  WT03       29 non-null     float64\n"," 21  WT08       40 non-null     float64\n","dtypes: float64(16), int64(3), object(3)\n","memory usage: 31.6+ KB\n","Data Info before Preprocessing None\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 183 entries, 0 to 182\n","Data columns (total 16 columns):\n"," #   Column   Non-Null Count  Dtype         \n","---  ------   --------------  -----         \n"," 0   STATION  183 non-null    object        \n"," 1   DATE     183 non-null    datetime64[ns]\n"," 2   AWND     183 non-null    float64       \n"," 3   PRCP     183 non-null    float64       \n"," 4   SNOW     183 non-null    float64       \n"," 5   SNWD     183 non-null    float64       \n"," 6   TMAX     183 non-null    int64         \n"," 7   TMIN     183 non-null    int64         \n"," 8   WDF2     183 non-null    int64         \n"," 9   WDF5     183 non-null    float64       \n"," 10  WSF2     183 non-null    float64       \n"," 11  WSF5     183 non-null    float64       \n"," 12  WT01     183 non-null    float64       \n"," 13  WT02     183 non-null    float64       \n"," 14  WT03     183 non-null    float64       \n"," 15  WT08     183 non-null    float64       \n","dtypes: datetime64[ns](1), float64(11), int64(3), object(1)\n","memory usage: 23.0+ KB\n","Data Info after Preprocessing None\n"]}],"source":["print('Data Info before Preprocessing', DailyWeather.info())\n","\n","DailyWeather.drop(['NAME', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'PGTM', 'TAVG'], axis=1, inplace=True)\n","DailyWeather['DATE'] = pd.to_datetime(DailyWeather['DATE'])\n","DailyWeather.fillna(0, inplace=True)\n","\n","print('Data Info after Preprocessing', DailyWeather.info())"]},{"cell_type":"markdown","metadata":{"id":"TgBto_HUDK8o"},"source":["## Debugging Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fm56FYXV3jbZ","outputId":"611635cf-5b61-4529-b732-da8c71b957c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 10000 entries, 3562429 to 1447365\n","Data columns (total 13 columns):\n"," #   Column              Non-Null Count  Dtype  \n","---  ------              --------------  -----  \n"," 0   ride_id             10000 non-null  object \n"," 1   rideable_type       10000 non-null  object \n"," 2   started_at          10000 non-null  object \n"," 3   ended_at            10000 non-null  object \n"," 4   start_station_name  9992 non-null   object \n"," 5   start_station_id    9992 non-null   object \n"," 6   end_station_name    9977 non-null   object \n"," 7   end_station_id      9977 non-null   object \n"," 8   start_lat           10000 non-null  float64\n"," 9   start_lng           10000 non-null  float64\n"," 10  end_lat             9992 non-null   float64\n"," 11  end_lng             9992 non-null   float64\n"," 12  member_casual       10000 non-null  object \n","dtypes: float64(4), object(9)\n","memory usage: 1.1+ MB\n","Data Info before Preprocessing None\n","<class 'pandas.core.frame.DataFrame'>\n","Index: 9972 entries, 3562429 to 1447365\n","Data columns (total 24 columns):\n"," #   Column                    Non-Null Count  Dtype         \n","---  ------                    --------------  -----         \n"," 0   ride_id                   9972 non-null   object        \n"," 1   rideable_type             9972 non-null   object        \n"," 2   started_at                9972 non-null   datetime64[ns]\n"," 3   ended_at                  9972 non-null   datetime64[ns]\n"," 4   start_station_name        9972 non-null   object        \n"," 5   start_station_id          9972 non-null   object        \n"," 6   end_station_name          9972 non-null   object        \n"," 7   end_station_id            9972 non-null   object        \n"," 8   start_lat                 9972 non-null   float64       \n"," 9   start_lng                 9972 non-null   float64       \n"," 10  end_lat                   9972 non-null   float64       \n"," 11  end_lng                   9972 non-null   float64       \n"," 12  member_casual             9972 non-null   object        \n"," 13  date                      9972 non-null   datetime64[ns]\n"," 14  year                      9972 non-null   int32         \n"," 15  month                     9972 non-null   int32         \n"," 16  week                      9972 non-null   UInt32        \n"," 17  day                       9972 non-null   int32         \n"," 18  weekday                   9972 non-null   int32         \n"," 19  weekday_name              9972 non-null   object        \n"," 20  hour                      9972 non-null   int32         \n"," 21  start_time                9972 non-null   object        \n"," 22  end_time                  9972 non-null   object        \n"," 23  trip_duration_in_minutes  9972 non-null   float64       \n","dtypes: UInt32(1), datetime64[ns](3), float64(5), int32(5), object(10)\n","memory usage: 1.7+ MB\n","Data Info after Preprocessing None\n"]}],"source":["print('Data Info before Preprocessing',debugging_df.info())\n","\n","debugging_df['started_at'] = pd.to_datetime(debugging_df['started_at'])\n","debugging_df['ended_at'] = pd.to_datetime(debugging_df['ended_at'])\n","\n","debugging_df['date'] = debugging_df['started_at'].dt.date\n","debugging_df['date'] = pd.to_datetime(debugging_df['date'])\n","\n","debugging_df['year'] = debugging_df['started_at'].dt.year\n","debugging_df['month'] = debugging_df['started_at'].dt.month\n","debugging_df['week'] = debugging_df['started_at'].dt.isocalendar().week\n","debugging_df['day'] = debugging_df['started_at'].dt.day\n","debugging_df['weekday'] = debugging_df['started_at'].dt.weekday\n","debugging_df['weekday_name'] = debugging_df['started_at'].dt.day_name()\n","debugging_df['hour'] = debugging_df['started_at'].dt.hour\n","\n","debugging_df['start_time'] = debugging_df['started_at'].dt.time\n","debugging_df['end_time'] = debugging_df['ended_at'].dt.time\n","\n","debugging_df['trip_duration_in_minutes'] = (debugging_df['ended_at'] - debugging_df['started_at']).dt.total_seconds() / 60\n","\n","debugging_df.dropna(inplace=True)\n","\n","print('Data Info after Preprocessing',debugging_df.info())"]},{"cell_type":"markdown","metadata":{"id":"UsJ5-yFMsby9"},"source":["### Merging Debugging Dataset with Weather Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YESy-dN5sZTI","outputId":"2e03e5ea-86e5-4aa2-b137-b8c4ffec7039"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 9972 entries, 0 to 9971\n","Data columns (total 38 columns):\n"," #   Column                    Non-Null Count  Dtype         \n","---  ------                    --------------  -----         \n"," 0   ride_id                   9972 non-null   object        \n"," 1   rideable_type             9972 non-null   object        \n"," 2   started_at                9972 non-null   datetime64[ns]\n"," 3   ended_at                  9972 non-null   datetime64[ns]\n"," 4   start_station_name        9972 non-null   object        \n"," 5   start_station_id          9972 non-null   object        \n"," 6   end_station_name          9972 non-null   object        \n"," 7   end_station_id            9972 non-null   object        \n"," 8   start_lat                 9972 non-null   float64       \n"," 9   start_lng                 9972 non-null   float64       \n"," 10  end_lat                   9972 non-null   float64       \n"," 11  end_lng                   9972 non-null   float64       \n"," 12  member_casual             9972 non-null   object        \n"," 13  date                      9972 non-null   datetime64[ns]\n"," 14  year                      9972 non-null   int32         \n"," 15  month                     9972 non-null   int32         \n"," 16  week                      9972 non-null   UInt32        \n"," 17  day                       9972 non-null   int32         \n"," 18  weekday                   9972 non-null   int32         \n"," 19  weekday_name              9972 non-null   object        \n"," 20  hour                      9972 non-null   int32         \n"," 21  start_time                9972 non-null   object        \n"," 22  end_time                  9972 non-null   object        \n"," 23  trip_duration_in_minutes  9972 non-null   float64       \n"," 24  AWND                      9972 non-null   float64       \n"," 25  PRCP                      9972 non-null   float64       \n"," 26  SNOW                      9972 non-null   float64       \n"," 27  SNWD                      9972 non-null   float64       \n"," 28  TMAX                      9972 non-null   int64         \n"," 29  TMIN                      9972 non-null   int64         \n"," 30  WDF2                      9972 non-null   int64         \n"," 31  WDF5                      9972 non-null   float64       \n"," 32  WSF2                      9972 non-null   float64       \n"," 33  WSF5                      9972 non-null   float64       \n"," 34  WT01                      9972 non-null   float64       \n"," 35  WT02                      9972 non-null   float64       \n"," 36  WT03                      9972 non-null   float64       \n"," 37  WT08                      9972 non-null   float64       \n","dtypes: UInt32(1), datetime64[ns](3), float64(16), int32(5), int64(3), object(10)\n","memory usage: 2.7+ MB\n"]}],"source":["debugging = pd.merge(debugging_df, DailyWeather, left_on='date', right_on='DATE', how='left')\n","debugging.drop(['DATE', 'STATION'], axis=1, inplace=True)\n","debugging.info()"]},{"cell_type":"markdown","metadata":{"id":"c1Tv6LxwsuxS"},"source":["### Saving and Loading Debugging Dataframe using Pickle File"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vqh5forG4RJS","outputId":"d5913efc-33d0-40cd-f296-d2731f4dcb59"},"outputs":[{"data":{"text/plain":["{'ResponseMetadata': {'RequestId': 'BJST9WAJBDRQ0YP1',\n","  'HostId': 'owOXusBHgLsncCuYHuRzq2tD4Bg6IsLbyCzaXZ9n9Ed36lqdQE0tJVH/UmV32SjX9NgPylRiljg=',\n","  'HTTPStatusCode': 200,\n","  'HTTPHeaders': {'x-amz-id-2': 'owOXusBHgLsncCuYHuRzq2tD4Bg6IsLbyCzaXZ9n9Ed36lqdQE0tJVH/UmV32SjX9NgPylRiljg=',\n","   'x-amz-request-id': 'BJST9WAJBDRQ0YP1',\n","   'date': 'Mon, 27 Nov 2023 03:27:14 GMT',\n","   'x-amz-server-side-encryption': 'AES256',\n","   'etag': '\"cf12be811c0a1692f1c8f1829e4b84c5\"',\n","   'server': 'AmazonS3',\n","   'content-length': '0'},\n","  'RetryAttempts': 0},\n"," 'ETag': '\"cf12be811c0a1692f1c8f1829e4b84c5\"',\n"," 'ServerSideEncryption': 'AES256'}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Convert the DataFrame to pickle format\n","pickle_data = BytesIO()\n","debugging.to_pickle(pickle_data)\n","\n","# Set the S3 path where you want to store the pickle file\n","s3_path = 'Dataset/debugging.pkl'\n","\n","# Upload the pickle data to S3\n","s3.put_object(Bucket=BUCKET_NAME, Key=s3_path, Body=pickle_data.getvalue())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"_0R49KDo8dAk","outputId":"5966c0cc-257c-4b37-c52e-9c4d3c1ae257"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ride_id</th>\n","      <th>rideable_type</th>\n","      <th>started_at</th>\n","      <th>ended_at</th>\n","      <th>start_station_name</th>\n","      <th>start_station_id</th>\n","      <th>end_station_name</th>\n","      <th>end_station_id</th>\n","      <th>start_lat</th>\n","      <th>start_lng</th>\n","      <th>...</th>\n","      <th>TMAX</th>\n","      <th>TMIN</th>\n","      <th>WDF2</th>\n","      <th>WDF5</th>\n","      <th>WSF2</th>\n","      <th>WSF5</th>\n","      <th>WT01</th>\n","      <th>WT02</th>\n","      <th>WT03</th>\n","      <th>WT08</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>FA36FE47D3A88A26</td>\n","      <td>classic_bike</td>\n","      <td>2023-09-09 18:38:34</td>\n","      <td>2023-09-09 18:57:42</td>\n","      <td>E 47 St &amp; 2 Ave</td>\n","      <td>6498.10</td>\n","      <td>Laight St &amp; Hudson St</td>\n","      <td>5539.06</td>\n","      <td>40.753406</td>\n","      <td>-73.970950</td>\n","      <td>...</td>\n","      <td>84</td>\n","      <td>70</td>\n","      <td>70</td>\n","      <td>60.0</td>\n","      <td>15.0</td>\n","      <td>25.1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7DA2BBBFACB65FEE</td>\n","      <td>classic_bike</td>\n","      <td>2023-09-11 19:28:31</td>\n","      <td>2023-09-11 19:46:35</td>\n","      <td>S 3 St &amp; Bedford Ave</td>\n","      <td>5235.05</td>\n","      <td>Lawrence St &amp; Willoughby St</td>\n","      <td>4596.09</td>\n","      <td>40.712564</td>\n","      <td>-73.962690</td>\n","      <td>...</td>\n","      <td>82</td>\n","      <td>69</td>\n","      <td>210</td>\n","      <td>220.0</td>\n","      <td>10.1</td>\n","      <td>15.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A34B90C2EDBDB134</td>\n","      <td>classic_bike</td>\n","      <td>2023-09-26 19:01:07</td>\n","      <td>2023-09-26 19:20:57</td>\n","      <td>5 Ave &amp; E 87 St</td>\n","      <td>7323.09</td>\n","      <td>E 114 St &amp; 1 Ave</td>\n","      <td>7540.02</td>\n","      <td>40.782576</td>\n","      <td>-73.959704</td>\n","      <td>...</td>\n","      <td>60</td>\n","      <td>54</td>\n","      <td>60</td>\n","      <td>30.0</td>\n","      <td>14.1</td>\n","      <td>19.9</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>97FEC210C1E0BA88</td>\n","      <td>classic_bike</td>\n","      <td>2023-09-20 07:18:40</td>\n","      <td>2023-09-20 07:20:09</td>\n","      <td>West Drive &amp; Prospect Park West</td>\n","      <td>3651.04</td>\n","      <td>Prospect Park West &amp; 8 St</td>\n","      <td>3722.04</td>\n","      <td>40.661218</td>\n","      <td>-73.979227</td>\n","      <td>...</td>\n","      <td>74</td>\n","      <td>58</td>\n","      <td>300</td>\n","      <td>250.0</td>\n","      <td>10.1</td>\n","      <td>15.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5428792F62754CE8</td>\n","      <td>classic_bike</td>\n","      <td>2023-09-19 18:54:43</td>\n","      <td>2023-09-19 19:18:35</td>\n","      <td>Canal St &amp; Rutgers St</td>\n","      <td>5303.08</td>\n","      <td>Wyckoff Av &amp; Jefferson St</td>\n","      <td>5051.01</td>\n","      <td>40.714311</td>\n","      <td>-73.989925</td>\n","      <td>...</td>\n","      <td>72</td>\n","      <td>58</td>\n","      <td>290</td>\n","      <td>270.0</td>\n","      <td>13.0</td>\n","      <td>19.9</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 38 columns</p>\n","</div>"],"text/plain":["            ride_id rideable_type          started_at            ended_at   \n","0  FA36FE47D3A88A26  classic_bike 2023-09-09 18:38:34 2023-09-09 18:57:42  \\\n","1  7DA2BBBFACB65FEE  classic_bike 2023-09-11 19:28:31 2023-09-11 19:46:35   \n","2  A34B90C2EDBDB134  classic_bike 2023-09-26 19:01:07 2023-09-26 19:20:57   \n","3  97FEC210C1E0BA88  classic_bike 2023-09-20 07:18:40 2023-09-20 07:20:09   \n","4  5428792F62754CE8  classic_bike 2023-09-19 18:54:43 2023-09-19 19:18:35   \n","\n","                start_station_name start_station_id   \n","0                  E 47 St & 2 Ave          6498.10  \\\n","1             S 3 St & Bedford Ave          5235.05   \n","2                  5 Ave & E 87 St          7323.09   \n","3  West Drive & Prospect Park West          3651.04   \n","4            Canal St & Rutgers St          5303.08   \n","\n","              end_station_name end_station_id  start_lat  start_lng  ...   \n","0        Laight St & Hudson St        5539.06  40.753406 -73.970950  ...  \\\n","1  Lawrence St & Willoughby St        4596.09  40.712564 -73.962690  ...   \n","2             E 114 St & 1 Ave        7540.02  40.782576 -73.959704  ...   \n","3    Prospect Park West & 8 St        3722.04  40.661218 -73.979227  ...   \n","4    Wyckoff Av & Jefferson St        5051.01  40.714311 -73.989925  ...   \n","\n","   TMAX  TMIN WDF2   WDF5  WSF2  WSF5  WT01  WT02  WT03 WT08  \n","0    84    70   70   60.0  15.0  25.1   1.0   0.0   1.0  1.0  \n","1    82    69  210  220.0  10.1  15.0   1.0   1.0   1.0  0.0  \n","2    60    54   60   30.0  14.1  19.9   1.0   0.0   0.0  0.0  \n","3    74    58  300  250.0  10.1  15.0   0.0   0.0   0.0  0.0  \n","4    72    58  290  270.0  13.0  19.9   0.0   0.0   0.0  0.0  \n","\n","[5 rows x 38 columns]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Load the data from the URL\n","url = \"https://s3-us-east-2.amazonaws.com/dhruvborda-project-nyccitibikerentals/Dataset/debugging.pkl\"\n","response = requests.get(url)\n","\n","if response.status_code == 200:\n","    debugging = pd.read_pickle(io.BytesIO(response.content))\n","    print(\"Data loaded successfully.\")\n","else:\n","    print(f\"Failed to download debugging.pkl. Status code: {response.status_code}\")\n","    exit()  # Exit if data loading fails\n","\n","debugging.head()"]},{"cell_type":"markdown","metadata":{"id":"rX27lxuuDOgO"},"source":["## Working Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9uxRVWONEoSJ","outputId":"54c6c9db-6ae9-444a-c4a3-47653dbfd167"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data Types before Preprocessing ride_id                object\n","rideable_type          object\n","started_at             object\n","ended_at               object\n","start_station_name     object\n","start_station_id       object\n","end_station_name       object\n","end_station_id         object\n","start_lat             float64\n","start_lng             float64\n","end_lat               float64\n","end_lng               float64\n","member_casual          object\n","dtype: object\n","Data Types before Preprocessing ride_id                             object\n","rideable_type                       object\n","started_at                  datetime64[ns]\n","ended_at                    datetime64[ns]\n","start_station_name                  object\n","start_station_id                    object\n","end_station_name                    object\n","end_station_id                      object\n","start_lat                          float64\n","start_lng                          float64\n","end_lat                            float64\n","end_lng                            float64\n","member_casual                       object\n","date                        datetime64[ns]\n","year                                 int32\n","month                                int32\n","week                                UInt32\n","day                                  int32\n","weekday                              int32\n","weekday_name                        object\n","hour                                 int32\n","start_time                          object\n","end_time                            object\n","trip_duration_in_minutes           float64\n","dtype: object\n"]}],"source":["print('Data Types before Preprocessing',working_df.dtypes)\n","\n","working_df['started_at'] = pd.to_datetime(working_df['started_at'])\n","working_df['ended_at'] = pd.to_datetime(working_df['ended_at'])\n","\n","working_df['date'] = working_df['started_at'].dt.date\n","working_df['date'] = pd.to_datetime(working_df['date'])\n","\n","working_df['year'] = working_df['started_at'].dt.year\n","working_df['month'] = working_df['started_at'].dt.month\n","working_df['week'] = working_df['started_at'].dt.isocalendar().week\n","working_df['day'] = working_df['started_at'].dt.day\n","working_df['weekday'] = working_df['started_at'].dt.weekday\n","working_df['weekday_name'] = working_df['started_at'].dt.day_name()\n","working_df['hour'] = working_df['started_at'].dt.hour\n","\n","working_df['start_time'] = working_df['started_at'].dt.time\n","working_df['end_time'] = working_df['ended_at'].dt.time\n","\n","working_df['trip_duration_in_minutes'] = (working_df['ended_at'] - working_df['started_at']).dt.total_seconds() / 60\n","\n","working_df.dropna(inplace=True)\n","\n","print('Data Types before Preprocessing',working_df.dtypes)"]},{"cell_type":"markdown","metadata":{"id":"hkJ9vhGmEoSM"},"source":["### Merging Working Dataset with Weather Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yGsMc0-IEoSN","outputId":"53913217-a211-4fd0-dec5-c658cb4e638f"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 997248 entries, 0 to 997247\n","Data columns (total 38 columns):\n"," #   Column                    Non-Null Count   Dtype         \n","---  ------                    --------------   -----         \n"," 0   ride_id                   997248 non-null  object        \n"," 1   rideable_type             997248 non-null  object        \n"," 2   started_at                997248 non-null  datetime64[ns]\n"," 3   ended_at                  997248 non-null  datetime64[ns]\n"," 4   start_station_name        997248 non-null  object        \n"," 5   start_station_id          997248 non-null  object        \n"," 6   end_station_name          997248 non-null  object        \n"," 7   end_station_id            997248 non-null  object        \n"," 8   start_lat                 997248 non-null  float64       \n"," 9   start_lng                 997248 non-null  float64       \n"," 10  end_lat                   997248 non-null  float64       \n"," 11  end_lng                   997248 non-null  float64       \n"," 12  member_casual             997248 non-null  object        \n"," 13  date                      997248 non-null  datetime64[ns]\n"," 14  year                      997248 non-null  int32         \n"," 15  month                     997248 non-null  int32         \n"," 16  week                      997248 non-null  UInt32        \n"," 17  day                       997248 non-null  int32         \n"," 18  weekday                   997248 non-null  int32         \n"," 19  weekday_name              997248 non-null  object        \n"," 20  hour                      997248 non-null  int32         \n"," 21  start_time                997248 non-null  object        \n"," 22  end_time                  997248 non-null  object        \n"," 23  trip_duration_in_minutes  997248 non-null  float64       \n"," 24  AWND                      997248 non-null  float64       \n"," 25  PRCP                      997248 non-null  float64       \n"," 26  SNOW                      997248 non-null  float64       \n"," 27  SNWD                      997248 non-null  float64       \n"," 28  TMAX                      997248 non-null  int64         \n"," 29  TMIN                      997248 non-null  int64         \n"," 30  WDF2                      997248 non-null  int64         \n"," 31  WDF5                      997248 non-null  float64       \n"," 32  WSF2                      997248 non-null  float64       \n"," 33  WSF5                      997248 non-null  float64       \n"," 34  WT01                      997248 non-null  float64       \n"," 35  WT02                      997248 non-null  float64       \n"," 36  WT03                      997248 non-null  float64       \n"," 37  WT08                      997248 non-null  float64       \n","dtypes: UInt32(1), datetime64[ns](3), float64(16), int32(5), int64(3), object(10)\n","memory usage: 267.2+ MB\n"]}],"source":["working = pd.merge(working_df, DailyWeather, left_on='date', right_on='DATE', how='left')\n","working.drop(['DATE', 'STATION'], axis=1, inplace=True)\n","working.info()"]},{"cell_type":"markdown","metadata":{"id":"sMJI9_qNEoSO"},"source":["### Saving and Loading Working Dataframe using Pickle File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XykRFB2zEoSO","outputId":"336f6f8b-7e2d-488c-ee52-4775acd02559"},"outputs":[{"data":{"text/plain":["{'ResponseMetadata': {'RequestId': 'JRVJPWB2A9B1ZRDX',\n","  'HostId': 'RcKf82ai9i+MXg9dxN6hHTho+9kHsLQh5/VrF9IUmDzo5hbHHqdGaGIckTJ7qzY5NQ2S7k5GwN0=',\n","  'HTTPStatusCode': 200,\n","  'HTTPHeaders': {'x-amz-id-2': 'RcKf82ai9i+MXg9dxN6hHTho+9kHsLQh5/VrF9IUmDzo5hbHHqdGaGIckTJ7qzY5NQ2S7k5GwN0=',\n","   'x-amz-request-id': 'JRVJPWB2A9B1ZRDX',\n","   'date': 'Mon, 27 Nov 2023 04:15:44 GMT',\n","   'x-amz-server-side-encryption': 'AES256',\n","   'etag': '\"64651154e613d7db00475fa9ba01fea1\"',\n","   'server': 'AmazonS3',\n","   'content-length': '0'},\n","  'RetryAttempts': 0},\n"," 'ETag': '\"64651154e613d7db00475fa9ba01fea1\"',\n"," 'ServerSideEncryption': 'AES256'}"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# Convert the DataFrame to pickle format\n","pickle_data = BytesIO()\n","working.to_pickle(pickle_data)\n","\n","# Set the S3 path where you want to store the pickle file\n","s3_path = 'Dataset/working.pkl'\n","\n","# Upload the pickle data to S3\n","s3.put_object(Bucket=BUCKET_NAME, Key=s3_path, Body=pickle_data.getvalue())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ad-a7YrREoSP","outputId":"b4787a5f-eebf-49ae-83bd-168812aed8f0"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ride_id</th>\n","      <th>rideable_type</th>\n","      <th>started_at</th>\n","      <th>ended_at</th>\n","      <th>start_station_name</th>\n","      <th>start_station_id</th>\n","      <th>end_station_name</th>\n","      <th>end_station_id</th>\n","      <th>start_lat</th>\n","      <th>start_lng</th>\n","      <th>...</th>\n","      <th>TMAX</th>\n","      <th>TMIN</th>\n","      <th>WDF2</th>\n","      <th>WDF5</th>\n","      <th>WSF2</th>\n","      <th>WSF5</th>\n","      <th>WT01</th>\n","      <th>WT02</th>\n","      <th>WT03</th>\n","      <th>WT08</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>E40E510853C33688</td>\n","      <td>classic_bike</td>\n","      <td>2023-07-26 23:25:09</td>\n","      <td>2023-07-26 23:30:37</td>\n","      <td>6 Ave &amp; Canal St</td>\n","      <td>5500.07</td>\n","      <td>Greenwich St &amp; Perry St</td>\n","      <td>5922.04</td>\n","      <td>40.722399</td>\n","      <td>-74.005724</td>\n","      <td>...</td>\n","      <td>87</td>\n","      <td>68</td>\n","      <td>200</td>\n","      <td>220.0</td>\n","      <td>8.9</td>\n","      <td>15.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>BF9D4640F2DDA41E</td>\n","      <td>classic_bike</td>\n","      <td>2023-04-24 13:06:46</td>\n","      <td>2023-04-24 13:18:06</td>\n","      <td>King St &amp; Varick St</td>\n","      <td>5687.11</td>\n","      <td>Gramercy Park N &amp; Gramercy Park E</td>\n","      <td>6013.12</td>\n","      <td>40.727890</td>\n","      <td>-74.005243</td>\n","      <td>...</td>\n","      <td>62</td>\n","      <td>44</td>\n","      <td>290</td>\n","      <td>290.0</td>\n","      <td>10.1</td>\n","      <td>16.1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>588E2D056D1C3CCB</td>\n","      <td>classic_bike</td>\n","      <td>2023-08-17 17:52:02</td>\n","      <td>2023-08-17 18:16:29</td>\n","      <td>William St &amp; Pine St</td>\n","      <td>5065.12</td>\n","      <td>Lexington Ave &amp; E 26 St</td>\n","      <td>6089.08</td>\n","      <td>40.706872</td>\n","      <td>-74.009108</td>\n","      <td>...</td>\n","      <td>79</td>\n","      <td>72</td>\n","      <td>120</td>\n","      <td>120.0</td>\n","      <td>12.1</td>\n","      <td>19.9</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9B205CFE74B13CAD</td>\n","      <td>classic_bike</td>\n","      <td>2023-07-04 10:41:26</td>\n","      <td>2023-07-04 10:55:32</td>\n","      <td>Lexington Ave &amp; E 26 St</td>\n","      <td>6089.08</td>\n","      <td>E 47 St &amp; 2 Ave</td>\n","      <td>6498.10</td>\n","      <td>40.741459</td>\n","      <td>-73.983293</td>\n","      <td>...</td>\n","      <td>83</td>\n","      <td>73</td>\n","      <td>230</td>\n","      <td>230.0</td>\n","      <td>10.1</td>\n","      <td>14.1</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>08A16B1DE7CA6DD9</td>\n","      <td>classic_bike</td>\n","      <td>2023-08-25 17:42:30</td>\n","      <td>2023-08-25 17:51:50</td>\n","      <td>Nostrand Ave &amp; Myrtle Ave</td>\n","      <td>4707.04</td>\n","      <td>Lafayette Ave &amp; Stuyvesant Ave</td>\n","      <td>4576.11</td>\n","      <td>40.695270</td>\n","      <td>-73.952381</td>\n","      <td>...</td>\n","      <td>78</td>\n","      <td>69</td>\n","      <td>200</td>\n","      <td>100.0</td>\n","      <td>8.9</td>\n","      <td>28.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 38 columns</p>\n","</div>"],"text/plain":["            ride_id rideable_type          started_at            ended_at   \n","0  E40E510853C33688  classic_bike 2023-07-26 23:25:09 2023-07-26 23:30:37  \\\n","1  BF9D4640F2DDA41E  classic_bike 2023-04-24 13:06:46 2023-04-24 13:18:06   \n","2  588E2D056D1C3CCB  classic_bike 2023-08-17 17:52:02 2023-08-17 18:16:29   \n","3  9B205CFE74B13CAD  classic_bike 2023-07-04 10:41:26 2023-07-04 10:55:32   \n","4  08A16B1DE7CA6DD9  classic_bike 2023-08-25 17:42:30 2023-08-25 17:51:50   \n","\n","          start_station_name start_station_id   \n","0           6 Ave & Canal St          5500.07  \\\n","1        King St & Varick St          5687.11   \n","2       William St & Pine St          5065.12   \n","3    Lexington Ave & E 26 St          6089.08   \n","4  Nostrand Ave & Myrtle Ave          4707.04   \n","\n","                    end_station_name end_station_id  start_lat  start_lng   \n","0            Greenwich St & Perry St        5922.04  40.722399 -74.005724  \\\n","1  Gramercy Park N & Gramercy Park E        6013.12  40.727890 -74.005243   \n","2            Lexington Ave & E 26 St        6089.08  40.706872 -74.009108   \n","3                    E 47 St & 2 Ave        6498.10  40.741459 -73.983293   \n","4     Lafayette Ave & Stuyvesant Ave        4576.11  40.695270 -73.952381   \n","\n","   ...  TMAX  TMIN WDF2   WDF5  WSF2  WSF5  WT01  WT02  WT03 WT08  \n","0  ...    87    68  200  220.0   8.9  15.0   1.0   0.0   0.0  0.0  \n","1  ...    62    44  290  290.0  10.1  16.1   0.0   0.0   0.0  0.0  \n","2  ...    79    72  120  120.0  12.1  19.9   1.0   0.0   0.0  1.0  \n","3  ...    83    73  230  230.0  10.1  14.1   1.0   1.0   1.0  1.0  \n","4  ...    78    69  200  100.0   8.9  28.0   1.0   0.0   0.0  1.0  \n","\n","[5 rows x 38 columns]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Load the data from the URL\n","url = \"https://s3-us-east-2.amazonaws.com/dhruvborda-project-nyccitibikerentals/Dataset/working.pkl\"\n","response = requests.get(url)\n","\n","if response.status_code == 200:\n","    working = pd.read_pickle(io.BytesIO(response.content))\n","    print(\"Data loaded successfully.\")\n","else:\n","    print(f\"Failed to download debugging.pkl. Status code: {response.status_code}\")\n","    exit()  # Exit if data loading fails\n","\n","working.head()"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["4T4ww9sw4JBg"]},"kernelspec":{"display_name":"tenet","language":"python","name":"tenet"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":0}